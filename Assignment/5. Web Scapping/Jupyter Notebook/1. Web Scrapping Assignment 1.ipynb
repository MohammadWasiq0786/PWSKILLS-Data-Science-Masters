{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e4d5428",
   "metadata": {},
   "source": [
    "![image](https://pwskills.com/images/PWSkills-main.png)\n",
    "\n",
    "# Data Science Masters\n",
    "\n",
    "## WEb Scrapping Assignments - 1\n",
    "\n",
    "### Image Scrapping\n",
    "\n",
    "### Date : 21 - Feb - 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79897169",
   "metadata": {},
   "source": [
    "**Q1 :** What is Web Scrapping ? Why is it Used?Give three areas where Web Scrapping is used to get data.\n",
    "\n",
    "**Ans :** Web scraping, also known as web harvesting or web data extraction, refers to the process of automatically extracting data from websites using software tools or scripts. Web scraping tools access the website's HTML code and extract the data contained within it, typically by parsing the HTML and using regular expressions or other algorithms to extract the desired information\n",
    "    \n",
    "Web scraping is used for a variety of purposes, including market research, price monitoring, data analytics, and content aggregation. By scraping data from multiple sources, businesses and researchers can gain insights into consumer behavior, track competitor pricing and promotions, and build databases of product information and customer reviews.\n",
    "    \n",
    "Here are three areas where web scraping is commonly used to get data:\n",
    "\n",
    "1. **E-Commerce :** Online retailers can use web scraping to gather pricing and product information from competitors, monitor customer reviews, and track changes in inventory levels. This information can help businesses stay competitive and make informed pricing and inventory decisions.\n",
    "\n",
    "2. **Data Analytics :** Web scraping can be used to collect data on social media activity, online news articles, and other web-based sources of information. This data can be analyzed to identify trends, track sentiment, and gain insights into consumer behavior.\n",
    "\n",
    "3. **Research :** Researchers in a variety of fields, including social sciences and humanities, can use web scraping to gather data for their studies. For example, researchers might scrape data from online forums or social media sites to analyze public opinion on a particular issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457ee8ee",
   "metadata": {},
   "source": [
    "**Q2 :** What are the different methods used for Web Scrapping ? \n",
    "\n",
    "**Ans :** There are several methods used for web scraping, depending on the type of data being extracted and the structure of the website being scraped. Here are some of the most common methods:\n",
    "\n",
    "* **Parsing HTML :** This method involves parsing the HTML code of a website to extract specific data using regular expressions or other parsing techniques. It is one of the simplest methods, but it can be limited by changes to the website's HTML structure.\n",
    "\n",
    "* **Web Scraping Libraries :** Developers often use web scraping libraries like Beautiful Soup, Scrapy, or Puppeteer, which provide pre-built functions and methods for accessing and scraping data from websites.\n",
    "\n",
    "* **APIs :** Some websites provide APIs (Application Programming Interfaces) that allow developers to access data in a structured format. APIs can be used to extract data in real-time, and they often provide more reliable and up-to-date data.\n",
    "\n",
    "* **Headless Browsers :** This method involves using headless browsers like PhantomJS or Selenium to automate web browsing and extract data from dynamic websites that require user interaction.\n",
    "\n",
    "* **Machine Learning :** Some web scraping tasks can be automated using machine learning algorithms that can identify patterns and extract data automatically. This method requires significant data preparation and training, but it can be more powerful and flexible than other methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fc4ec6",
   "metadata": {},
   "source": [
    "**Q3 :** What is Beautiful Soup ? Why is it used ?\n",
    "\n",
    "**Ans :** Beautiful Soup is a Python library used for web scraping purposes. It provides a simple and efficient way to parse HTML and XML documents, extract data, and navigate the document tree. Beautiful Soup can handle poorly formatted HTML, making it a popular choice among web developers and data analysts.\n",
    "\n",
    "Here are some of the reasons why Beautiful Soup is widely used for web scraping:\n",
    "\n",
    "* **Easy to learn and use :** Beautiful Soup has a user-friendly syntax and a simple API that makes it easy for beginners to start scraping data from websites.\n",
    "\n",
    "* **Robust parsing capabilities :** Beautiful Soup can parse even the most poorly formatted HTML, making it a flexible tool for web scraping.\n",
    "\n",
    "* **Navigation and search :** Beautiful Soup provides a range of search and navigation methods to locate and extract data from specific parts of an HTML document.\n",
    "\n",
    "*  **Integration with other libraries :** Beautiful Soup can be integrated with other Python libraries, such as requests and pandas, to handle HTTP requests and manage scraped data.\n",
    "\n",
    "* **Open source and community-driven :** Beautiful Soup is an open-source library, meaning that it is freely available and maintained by a large community of developers. This ensures that the library is continuously updated and improved with new features and bug fixes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d622d276",
   "metadata": {},
   "source": [
    "**Q4 :** Why is flask used in this Web Scraping project?\n",
    "\n",
    "**Ans :** Here are some of the reasons why Flask is commonly used in web scraping projects:\n",
    "\n",
    "* **Easy to set up :** Flask can be installed quickly and easily using pip, and requires minimal configuration to get started.\n",
    "* **Flexible routing :** Flask provides a simple and intuitive routing system that allows developers to map URLs to Python functions, making it easy to build web scrapers that follow a specific pattern or logic.\n",
    "* **Lightweight and modular :** Flask is a lightweight and modular framework that can be easily extended with third-party libraries and plugins, making it easy to add functionality to your web scraper as needed.\n",
    "* **Integration with other Python libraries :** Flask integrates seamlessly with other popular Python libraries, such as Beautiful Soup, Requests, and Pandas, making it easy to build a web scraper that leverages the power of these libraries.\n",
    "* **Easy to deploy :** Flask applications can be easily deployed to a wide range of hosting services, including Heroku, Google Cloud, and Amazon Web Services."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a52e496",
   "metadata": {},
   "source": [
    "**Q5 :** Write the names of AWS services used in this project. Also, explain the use of each service.\n",
    "\n",
    "**Ans :** The AWS services used in this project are  1. AWS  Codepipeline    2. AWS Elastic Beanstalk\n",
    "     \n",
    "AWS CodePipeline is a fully managed continuous delivery service that helps automate the release process for your web applications. It provides a set of tools and services that allow developers to build, test, and deploy their code automatically, from source code changes to production deployment. CodePipeline integrates with a wide range of AWS services, including Elastic Beanstalk, to provide a seamless and automated software release process.\n",
    "\n",
    "AWS Elastic Beanstalk is a fully managed service that makes it easy to deploy and run web applications and services on AWS. It provides an easy-to-use interface that allows developers to deploy web applications quickly and easily, without worrying about the underlying infrastructure. Elastic Beanstalk supports multiple languages, including Python, Ruby, Java, and Node.js, and provides a range of pre-configured environments that developers can use to deploy their applications.\n",
    "\n",
    "When used together, AWS CodePipeline and Elastic Beanstalk can simplify the software release process by automating the deployment of web applications to the cloud. Developers can use CodePipeline to define a release pipeline that includes multiple stages, such as build, test, and deploy, and configure Elastic Beanstalk to deploy the application to the appropriate environment based on the pipeline status. This allows developers to release new features and updates more quickly and with less risk, as the entire process is automated and standardized."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
