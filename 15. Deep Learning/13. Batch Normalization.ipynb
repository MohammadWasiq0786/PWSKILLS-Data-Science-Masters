{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88167d66",
   "metadata": {},
   "source": [
    "![image](https://pwskills.com/images/PWSkills-main.png)\n",
    "\n",
    "# Data Science Masters\n",
    "\n",
    "## Day - 108\n",
    "## Date- 24 May 2023 \n",
    "## Deep Learning - XIII\n",
    "\n",
    "### Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e118c1e7",
   "metadata": {},
   "source": [
    "# Batch normalization\n",
    "\n",
    "### Issues with training Deep Neural Networks \n",
    "\n",
    "- There are 2 major issues 1) Internal Covariate shift, 2) Vanishing Gradient\n",
    "\n",
    "### Internal Covariate shift\n",
    "\n",
    "- The concept of covariate shift pertains to the change that occurs in the distribution of the input to a learning system. In deep networks, this distribution can be influenced by parameters across all input layers. Consequently, even minor changes in the network can have a significant impact on its output. This effect gets magnified as the signal propagates through the network, which can result in a shift in the distribution of the inputs to internal layers. This phenomenon is known as internal covariate shift.\n",
    "\n",
    "- When inputs are whitened (i.e., have zero mean and unit variance) and are uncorrelated, they tend to converge faster during training. However, internal covariate shift can have the opposite effect, as it introduces changes to the distribution of inputs that can slow down convergence. Therefore, to mitigate this effect, techniques like batch normalization have been developed to normalize the inputs to each layer in the network based on statistics of the current mini-batch.\n",
    "\n",
    "### Vanishing Gradient\n",
    "\n",
    "- Saturating non-linearities such as sigmoid or tanh are not suitable for deep networks, as the signal tends to get trapped in the saturation region as the network grows deeper. This makes it difficult for the network to learn and can result in slow convergence during training. To overcome this problem we can use the following.\n",
    "\n",
    "- Non-linearities like ReLU which do not saturate.\n",
    "- Smaller learning rates\n",
    "- Careful initializations\n",
    "---\n",
    "### What is Normalization?\n",
    "\n",
    "- Normalization in deep learning refers to the process of transforming the input or output of a layer in a neural network to improve its performance during training. The most common type of normalization used in deep learning is batch normalization, which normalizes the activations of a layer for each mini-batch during training.\n",
    "---\n",
    "### What is batch normalization?\n",
    "\n",
    "- Batch normalization is a technique in deep learning that helps to standardize and normalize the input to each layer of a neural network by adjusting and scaling the activations. The idea behind batch normalization is to normalize the inputs to a layer to have zero mean and unit variance across each mini-batch of the training data.\n",
    "\n",
    "### Steps involved in batch normalization\n",
    "\n",
    "1. During training, for each mini-batch of data, compute the mean and variance of the activations of each layer. This can be done using the following formulas:\n",
    "\n",
    "- Mean: $\\mu_B = \\frac{1}{m} \\sum_{i=1}^m x_i$\n",
    "\n",
    "- Variance: $\\sigma_B^2 = \\frac{1}{m} \\sum_{i=1}^m (x_i - \\mu_B)^2$\n",
    "\n",
    "- Here, $m$ is the size of the mini-batch, and $x_i$ is the activation of the $i$-th neuron in the layer.\n",
    "\n",
    "2. Normalize the activations of each layer in the mini-batch using the following formula:\n",
    "\n",
    "- $\\hat{x_i} = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}$\n",
    "Here, $\\epsilon$ is a small constant added for numerical stability.\n",
    "\n",
    "3. Scale and shift the normalized activations using the learned parameters $\\gamma$ and $\\beta$, respectively:\n",
    "\n",
    "- $y_i = \\gamma \\hat{x_i} + \\beta$\n",
    "- The parameters $\\gamma$ and $\\beta$ are learned during training using backpropagation.\n",
    "\n",
    "4. During inference, the running mean and variance of each layer are used for normalization instead of the mini-batch statistics. These running statistics are updated using a moving average of the mini-batch statistics during training.\n",
    "\n",
    "---\n",
    "\n",
    "### The benefits of batch normalization include:\n",
    "\n",
    "- Improved training performance: Batch normalization reduces the internal covariate shift, which is the change in the distribution of the activations of each layer due to changes in the distribution of the inputs. This allows the network to converge faster and with more stable gradients.\n",
    "\n",
    "- Regularization: Batch normalization acts as a form of regularization by adding noise to the activations of each layer, which can help prevent overfitting.\n",
    "\n",
    "- Increased robustness: Batch normalization makes the network more robust to changes in the input distribution, which can help improve its generalization performance.\n",
    "---\n",
    "### Code example for batch normalization\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define a fully connected layer\n",
    "fc_layer = tf.keras.layers.Dense(units=128, activation='relu')\n",
    "\n",
    "# Add batch normalization to the layer\n",
    "bn_layer = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "# Define the model with the layer and batch normalization\n",
    "model = tf.keras.models.Sequential([fc_layer, bn_layer])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=10)\n",
    "```\n",
    "\n",
    "- In the above code,the tf.keras.layers.BatchNormalization() layer is added after the fully connected layer to normalize the output before passing it to the activation function. The model.fit() function is then used to train the model using batch normalization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
